# Code Repository for PIL-Net: Physics-Informed Graph Convolutional Network for Predicting Atomic Multipoles
### This code allows for running the PIL-Net machine learning pipeline on the QM Dataset for Atomic Multipoles.

## PIL-Net Modules:

### Classes:

`PILNet/PILNet/model/PILNet.py`<br>
Contains class definition for PIL-Net model architecture.

`PILNet/PILNet/model/PILNet_Conv.py`<br>
Contains class definition for PIL-Net model convolutional layer.

### Machine Learning Pipeline:

`PILNet/PILNet/preprocessing/ExtractFeatures.py`<br>
Read in dataset information, extract features, represent each molecule as a discrete graph, and save each graph and corresponding multipole moment labels.

`PILNet/PILNet/preprocessing/SplitData_NormalizeFeatures.py`<br>
Separate data into train/validation/test set splits and normalize features.

`PILNet/PILNet/preprocessing/ComputeReferenceMultipoles.py`<br>
Use PSI4 library to compute reference molecular quadrupole and octupole multipole moments (unavailable in QMDFAM dataset)
for a portion of the test set graphs.

`PILNet/PILNet/training/TrainNetwork.py`<br>
Train a PILNet model using the training and validation dataset splits.

`PILNet/PILNet/inference/PredictNetwork.py`<br>
Use the trained PILNet model(s) to predict the atomic and molecular multipole moment test set labels, 
as well as approximate the corresponding electrostatic potential.

## Data:
The `PILNet/data/` subdirectory contains three additional subdirectories `dataset/`, `graphs/`, and `splits/`. 

The QM Dataset for Atomic Multipoles (QMDFAM) is publicly accessible here: https://doi.org/10.3929/ethz-b-000509052.<br>
The `data.hdf5`, `data_gdb.hdf5`, and `data_esp.hdf5` dataset files must be downloaded prior to running the PIL-Net machine learning pipeline.<br>
Once downloaded, these files should be placed in the `PILNet/data/dataset/` subdirectory.<br>

The `graphs/` subdirectory is intended to hold the graph-formatted data generated by `ExtractFeatures.py`. The `splits/` subdirectory is intended to hold the train/validation/test set splits generated by `SplitData_Normalize.py` and later modified by `ComputeReferenceMultipoles.py`.

## Provided Trained PIL-Net Models:
### PILNet/saved_models/trained_models/
This subdirectory contains three trained PILNet "PINN" models.<br>
Each model is trained on the same training set split, but each uses a differently seeded randomization for the initial model weights.<br>
These models can be used for inference in lieu of running `TrainNetwork.py`.

`pilnet_model1.bin`<br>
`pilnet_model2.bin`<br>
`pilnet_model3.bin`<br>

## Instructions: Installing Package Dependencies

The PIL-Net code has been tested on a `Linux machine` with `CUDA version 12.1.1` and `Anaconda version 2024.09`.<br>
A bash script `install.sh` has been provided in the top-level `PILNet` directory to install the package dependencies required for running the PIL-Net pipeline.<br>
Prior to running the script, the user must download and install `Anaconda` because the script creates a `conda` environment `env_pilnet_pkg` and installs the packages into that environment.<br>

Subsequently, the script can be run as:
<pre>./install.sh</pre>

## Instructions: Running PIL-Net Code (Quickstart)

A Juypter notebook `model_evaluation_script.ipynb` has been provided in the `PILNet/scripts/` subdirectory.<br>
This notebook can be used to evaluate trained PIL-Net model(s) on the QMDFAM test set and plot the results.<br>
The notebook should be run in the same `conda` environment that was created with `install.sh`.<br>
The PIL-Net graph-formatted QMDFAM test dataset has been uploaded here: https://figshare.com/s/8cb46a680f883e96627f<br> 
This `testdata.bin` file should be downloaded and placed in the `PILNet/data/splits/` subdirectory prior to running the Jupyter notebook.<br>
The trained PIL-Net models are already provided in the `PILNet/saved_models/trained_models/` subdirectory.<br>

## Instructions: Running PIL-Net Code (Full Pipeline)

After the QMDFAM dataset files have been downloaded and the PIL-Net package dependencies have been installed, the user can run the full PIL-Net machine learning pipeline.<br>
A file `run_example.py` has been provided in the top-level `PILNet/` directory for that purpose.<br>
Running this program will execute the full PIL-Net pipeline, including data preprocessing, model training, and model inference.<br>

On a `Linux machine`, the `run_example.py` program can be run as:

<pre>
module load cuda/12.1.1
module load external
module load conda/2024.09
conda activate env_pilnet_pkg
export LD_PRELOAD=$CONDA_PREFIX/lib/libstdc++.so.6
python run_example.py
</pre>

## Function Arguments in run_example.py

Thie following function arguments are used in `run_example.py`:

### 1. ExtractFeatures.py
read_filepath: Path to directory with .hdf5 files containing QMDFAM multipole moment data.<br>
save_filepath: Path to directory to save DGL graphs of QMDFAM data and corresponding labels.<br>

### 2. SplitData_NormalizeFeatures.py
read_filepath: Path to directory containing files with DGL graphs and labels.<br>
save_filepath: Path to directory to save training/validation/test set splits of DGL graphs.<br>

### 3. ComputeReferenceMultipoles.py
read_filepath: Path to directory containing test set split of DGL graphs.<br>
save_filepath: Path to directory to save updated test set split of DGL graphs (new reference labels).<br>

### 4. TrainNetwork.py
read_filepath: Path to directory containing training/validation/test set splits.<br>
save_filepath: Path to directory to save trained pytorch model.<br>

### 5. PredictNetwork.py
read_filepath_splits: Path to directory containing training/validation/test splits and
the file containing the test set indices selected in ComputeReferenceMultipoles.py.<br>
read_filepath_esp: Path to directory containing QMDFAM electrostatic potential data.<br>
read_filepath_model: Path to directory to trained PIL-Net model(s).<br>

Note: All `.bin` files at the top-level inside the specified `read_filepath_model` subdirectory will be used for model inference.<br>
Their collective predictive error will be averaged.